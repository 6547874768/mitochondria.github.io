#!/usr/bin/env python3
"""
–°–∫–∞–Ω–µ—Ä –ø–∞—Ä—Ç–Ω—ë—Ä—Å–∫–∏—Ö —Å—Å—ã–ª–æ–∫
–ù–∞—Ö–æ–¥–∏—Ç –í–°–ï —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–∞—Ä—Ç–Ω—ë—Ä—Å–∫–∏–µ –¥–æ–º–µ–Ω—ã –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ
"""

import os
import re
from urllib.parse import urlparse
from collections import defaultdict
import json

def find_all_external_links():
    """–ù–∞–π—Ç–∏ –≤—Å–µ –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –≤–æ –≤—Å–µ—Ö HTML —Ñ–∞–π–ª–∞—Ö"""
    
    # –°–ª–æ–≤–∞—Ä—å: –¥–æ–º–µ–Ω -> {—Å—Ç—Ä–∞–Ω–∏—Ü—ã, –≥–¥–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è}
    domains = defaultdict(set)
    
    # –°–ª–æ–≤–∞—Ä—å: –¥–æ–º–µ–Ω -> –ø—Ä–∏–º–µ—Ä—ã —Å—Å—ã–ª–æ–∫
    domain_examples = defaultdict(list)
    
    # –°—á—ë—Ç—á–∏–∫–∏
    total_links = 0
    total_pages = 0
    
    print("üîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö HTML —Ñ–∞–π–ª–æ–≤...")
    print("=" * 60)
    
    # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º HTML —Ñ–∞–π–ª–∞–º
    for root, dirs, files in os.walk('.'):
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ø–∞–ø–∫–∏
        if any(skip in root for skip in ['.git', '.github', 'node_modules']):
            continue
            
        for file in files:
            if file.endswith('.html'):
                filepath = os.path.join(root, file)
                total_pages += 1
                
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ (href –∏ onclick)
                    # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è href
                    href_pattern = r'href=["\']([^"\']+)["\']'
                    href_links = re.findall(href_pattern, content, re.IGNORECASE)
                    
                    # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è onclick –∏ JavaScript
                    onclick_pattern = r'(?:window\.open|window\.location|location\.href|redirectToOffer)\s*\(\s*["\']([^"\']+)["\']'
                    onclick_links = re.findall(onclick_pattern, content, re.IGNORECASE)
                    
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                    all_links = href_links + onclick_links
                    
                    for link in all_links:
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏
                        if link.startswith('#') or link.startswith('/') or link.startswith('../'):
                            continue
                        
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º mailto –∏ tel
                        if link.startswith('mailto:') or link.startswith('tel:'):
                            continue
                        
                        # –ü–∞—Ä—Å–∏–º URL
                        if link.startswith('http'):
                            total_links += 1
                            parsed = urlparse(link)
                            domain = parsed.netloc.lower()
                            
                            # –£–±–∏—Ä–∞–µ–º www.
                            if domain.startswith('www.'):
                                domain = domain[4:]
                            
                            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–∞—à —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –¥–æ–º–µ–Ω
                            if 'danilichev.info' in domain or 'github' in domain:
                                continue
                            
                            # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ª–æ–≤–∞—Ä–∏
                            if domain:
                                domains[domain].add(filepath.replace('./', ''))
                                
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–º–µ—Ä —Å—Å—ã–ª–∫–∏ (–º–∞–∫—Å–∏–º—É–º 3 –ø—Ä–∏–º–µ—Ä–∞)
                                if len(domain_examples[domain]) < 3:
                                    domain_examples[domain].append(link)
                
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {filepath}: {e}")
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–æ–º–µ–Ω—ã –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏
    sorted_domains = sorted(domains.items(), key=lambda x: len(x[1]), reverse=True)
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   –ü—Ä–æ—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")
    print(f"   –ù–∞–π–¥–µ–Ω–æ –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫: {total_links}")
    print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤: {len(domains)}")
    print("=" * 60)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω—ã–µ –ø–∞—Ä—Ç–Ω—ë—Ä—Å–∫–∏–µ –¥–æ–º–µ–Ω—ã
    affiliate_keywords = [
        'shop', 'store', 'buy', 'affiliate', 'partner', 'offer',
        'promo', 'deal', 'sale', 'discount', 'amazon', 'digistore',
        'clickbank', 'shareasale', 'commission', 'ref', 'aff',
        'enhanced', 'nutritional', 'supplement', 'vitamin', 'health',
        'fitness', 'keto', 'diet', 'weight', 'muscle'
    ]
    
    probable_affiliates = []
    possible_affiliates = []
    
    print("\nüéØ –ù–ê–ô–î–ï–ù–ù–´–ï –î–û–ú–ï–ù–´ (–æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏):\n")
    
    for domain, pages in sorted_domains:
        page_count = len(pages)
        examples = domain_examples[domain]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—Ö–æ–∂ –ª–∏ –¥–æ–º–µ–Ω –Ω–∞ –ø–∞—Ä—Ç–Ω—ë—Ä—Å–∫–∏–π
        is_probable = any(keyword in domain.lower() for keyword in affiliate_keywords)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø–∞—Ä—Ç–Ω—ë—Ä—Å–∫–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –ø—Ä–∏–º–µ—Ä–∞—Ö
        has_aff_params = any(
            'aff=' in ex or 'ref=' in ex or 'affiliate' in ex or 
            'partner' in ex or 'utm_' in ex or 'click' in ex 
            for ex in examples
        )
        
        if is_probable or has_aff_params:
            probable_affiliates.append(domain)
            status = "‚≠ê –ü–ê–†–¢–ù–Å–†–°–ö–ò–ô"
        else:
            possible_affiliates.append(domain)
            status = "‚ùì –í–æ–∑–º–æ–∂–Ω–æ –ø–∞—Ä—Ç–Ω—ë—Ä—Å–∫–∏–π"
        
        print(f"{status}: {domain}")
        print(f"   üìÑ –í—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –Ω–∞ {page_count} —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö")
        print(f"   üìé –ü—Ä–∏–º–µ—Ä—ã —Å—Å—ã–ª–æ–∫:")
        for ex in examples[:2]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –º–∞–∫—Å–∏–º—É–º 2 –ø—Ä–∏–º–µ—Ä–∞
            if len(ex) > 100:
                ex = ex[:100] + "..."
            print(f"      ‚Ä¢ {ex}")
        print()
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è affiliate-tracking.js
    print("=" * 60)
    print("\nüìù –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –î–õ–Ø affiliate-tracking.js:\n")
    print("// –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –º–∞—Å—Å–∏–≤ –≤ –≤–∞—à affiliate-tracking.js")
    print("const AFFILIATE_URLS = [")
    
    # –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω—ã–µ –ø–∞—Ä—Ç–Ω—ë—Ä—Å–∫–∏–µ
    for domain in probable_affiliates:
        print(f"    '{domain}',")
        if not domain.startswith('www.'):
            print(f"    'www.{domain}',")
    
    print("    ")
    print("    // –í–æ–∑–º–æ–∂–Ω—ã–µ –ø–∞—Ä—Ç–Ω—ë—Ä—Å–∫–∏–µ (–ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∏ —É–¥–∞–ª–∏—Ç–µ –ª–∏—à–Ω–∏–µ)")
    for domain in possible_affiliates[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-10
        print(f"    // '{domain}',")
    
    print("];")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç –≤ JSON
    report = {
        "scan_date": str(os.popen('date').read().strip()),
        "total_pages": total_pages,
        "total_external_links": total_links,
        "unique_domains": len(domains),
        "probable_affiliates": probable_affiliates,
        "all_domains": {
            domain: {
                "pages_count": len(pages),
                "pages": list(pages)[:10],  # –ú–∞–∫—Å–∏–º—É–º 10 —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
                "examples": domain_examples[domain]
            }
            for domain, pages in sorted_domains
        }
    }
    
    with open('affiliate_scan_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print("\nüíæ –ü–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ affiliate_scan_report.json")
    print("=" * 60)
    
    # –í—ã–≤–æ–¥–∏–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
    print("\nüìã –ß–¢–û –î–ï–õ–ê–¢–¨ –î–ê–õ–¨–®–ï:")
    print("1. –ü—Ä–æ—Å–º–æ—Ç—Ä–∏—Ç–µ —Å–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω–æ–≤ –≤—ã—à–µ")
    print("2. –°–∫–æ–ø–∏—Ä—É–π—Ç–µ AFFILIATE_URLS –≤ –≤–∞—à affiliate-tracking.js")
    print("3. –£–¥–∞–ª–∏—Ç–µ –¥–æ–º–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï —è–≤–ª—è—é—Ç—Å—è –ø–∞—Ä—Ç–Ω—ë—Ä—Å–∫–∏–º–∏")
    print("4. –î–æ–±–∞–≤—å—Ç–µ –¥–æ–º–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ —è –º–æ–≥ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å")
    print("5. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–∞–π—Ç–∞ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π")
    
    return probable_affiliates

if __name__ == "__main__":
    find_all_external_links()
