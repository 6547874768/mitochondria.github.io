name: Auto-generate Sitemap + Robots

on:
  push:
    branches: [ main, master ]
  workflow_dispatch:

jobs:
  update-site:
    runs-on: ubuntu-latest

    permissions:
      contents: write

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Generate sitemap + robots.txt
      run: |
        python3 -c "
        import os
        import datetime

        SITE_URL = 'https://www.danilichev.info'

        def generate_sitemap():
            today = datetime.date.today().strftime('%Y-%m-%d')
            pages = []

            # –î–æ–±–∞–≤–∏–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            pages.append({
                'url': '/',
                'priority': '1.0',
                'changefreq': 'weekly'
            })

            # –î–æ–±–∞–≤–∏–º –≤—Å–µ index.html –≤ –ø–æ–¥–ø–∞–ø–∫–∞—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä /category/index.html => /category/)
            for root, dirs, files in os.walk('.'):
                dirs[:] = [d for d in dirs if not d.startswith('.')]
                if 'index.html' in files and root != '.':
                    rel_path = os.path.relpath(root, '.')
                    url_path = '/' + rel_path.replace('\\\\', '/') + '/'
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
                    category_folders = ['health-supplements', 'health-products', 'anti-aging-hacks', 'brain-supplements', 'weight-loss-supplements']
                    priority = '0.9' if any(cat in rel_path for cat in category_folders) else '0.8'
                    
                    pages.append({
                        'url': url_path,
                        'priority': priority,
                        'changefreq': 'weekly'
                    })

            # –î–æ–±–∞–≤–∏–º –≤—Å–µ –æ—Ç–¥–µ–ª—å–Ω—ã–µ .html —Ñ–∞–π–ª—ã (—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–ª–∏ —Å—Ç–∞—Ç—å–∏)
            for root, dirs, files in os.walk('.'):
                dirs[:] = [d for d in dirs if not d.startswith('.')]
                for file in files:
                    if file.endswith('.html') and file not in ['index.html', '404.html']:
                        rel_path = os.path.relpath(os.path.join(root, file), '.')
                        url_path = '/' + rel_path.replace('\\\\', '/')
                        
                        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ—Å—Ç–∞—é—Ç—Å—è –∫–∞–∫ .html
                        is_technical = any(kw in file for kw in [
                            'affiliate-disclosure', 'contact-us', 'disclaimer',
                            'editorial-policy', 'privacy-policy', 'terms-of-use',
                            'sitemap', 'about-us'
                        ])
                        
                        if is_technical:
                            # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: –æ—Å—Ç–∞–≤–ª—è–µ–º .html
                            priority = '0.7'
                            url_final = url_path
                        else:
                            # –ö–æ–Ω—Ç–µ–Ω—Ç–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: —É–±–∏—Ä–∞–µ–º .html, –¥–æ–±–∞–≤–ª—è–µ–º /
                            priority = '0.8'
                            url_final = url_path[:-5] + '/' if url_path.endswith('.html') else url_path
                        
                        pages.append({
                            'url': url_final,
                            'priority': priority,
                            'changefreq': 'monthly'
                        })

            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            missing_pages = [
                '/best-energy-supplements-after-40/',
                '/cellular-energy-production-slowdown/',
                '/low-energy-warning-signs-over-40/',
                '/mens-fatigue-solutions-over-40/',
                '/mitochondrial-dysfunction-aging-process/',
                '/mitochondrial-supplements-research/',
                '/nad-boosting-supplements-comparison/',
                '/nad-decline-after-40-effects/',
                '/natural-energy-restoration-methods/',
                '/womens-energy-decline-menopause/'
            ]
            
            for missing_url in missing_pages:
                pages.append({
                    'url': missing_url,
                    'priority': '0.8',
                    'changefreq': 'monthly'
                })

            # –£–¥–∞–ª–∏–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
            seen = set()
            unique_pages = []
            for p in pages:
                if p['url'] not in seen:
                    unique_pages.append(p)
                    seen.add(p['url'])
            pages = sorted(unique_pages, key=lambda x: float(x['priority']), reverse=True)

            # –°–±–æ—Ä–∫–∞ XML —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
            xml_content = []
            xml_content.append('<?xml version=\"1.0\" encoding=\"UTF-8\"?>')
            xml_content.append('<urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"')
            xml_content.append('        xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"')
            xml_content.append('        xsi:schemaLocation=\"http://www.sitemaps.org/schemas/sitemap/0.9')
            xml_content.append('        http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd\">')
            xml_content.append('')  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
            
            for page in pages:
                xml_content.append('  <url>')
                xml_content.append(f'    <loc>{SITE_URL}{page[\"url\"]}</loc>')
                xml_content.append(f'    <lastmod>{today}</lastmod>')
                xml_content.append(f'    <changefreq>{page[\"changefreq\"]}</changefreq>')
                xml_content.append(f'    <priority>{page[\"priority\"]}</priority>')
                xml_content.append('  </url>')
            
            xml_content.append('</urlset>')
            
            with open('sitemap.xml', 'w', encoding='utf-8') as f:
                f.write('\\n'.join(xml_content))
            print(f'‚úÖ sitemap.xml —Å–æ–∑–¥–∞–Ω: {len(pages)} —Å—Ç—Ä–∞–Ω–∏—Ü')

            # –°–æ–∑–¥–∞–µ–º robots.txt
            with open('robots.txt', 'w', encoding='utf-8') as f:
                f.write(f'User-agent: *\\nAllow: /\\n\\nSitemap: {SITE_URL}/sitemap.xml')
            print('‚úÖ robots.txt —Å–æ–∑–¥–∞–Ω')

        def main():
            print('üöÄ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è sitemap.xml –∏ robots.txt...')
            generate_sitemap()
            print('‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!')

        if __name__ == '__main__':
            main()

        main()
        "

    - name: Check for changes
      id: check_changes
      run: |
        if git diff --quiet; then
          echo "changed=false" >> $GITHUB_OUTPUT
          echo "No changes detected"
        else
          echo "changed=true" >> $GITHUB_OUTPUT
          echo "Changes detected"
        fi

    - name: Commit and push changes
      if: steps.check_changes.outputs.changed == 'true'
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "GitHub Actions"
        git add sitemap.xml robots.txt
        git commit -m "üîÑ Auto-regenerated sitemap.xml and robots.txt

        üìä Complete sitemap with all pages
        üîß Fixed XML structure with proper <urlset> tags
        ‚úÖ Technical pages priority 0.7, content pages 0.8
        ü§ñ Updated robots.txt with sitemap reference"
        git push

    - name: No changes
      if: steps.check_changes.outputs.changed == 'false'
      run: echo "üü¢ –í—Å—ë –∞–∫—Ç—É–∞–ª—å–Ω–æ. –û–±–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–µ –Ω—É–∂–Ω—ã."
