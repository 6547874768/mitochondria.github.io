name: Auto-generate Sitemap + Robots

on:
  push:
    branches: [ main, master ]
  workflow_dispatch:

jobs:
  update-site:
    runs-on: ubuntu-latest

    permissions:
      contents: write

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Generate sitemap + robots.txt
      run: |
        cat > update_site.py << 'EOF'
        import os
        import datetime

        SITE_URL = "https://www.danilichev.info"

        def generate_sitemap():
            today = datetime.date.today().strftime('%Y-%m-%d')
            pages = []

            # –î–æ–±–∞–≤–∏–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            pages.append({
                'url': '/',
                'priority': '1.0',
                'changefreq': 'weekly'
            })

            # –î–æ–±–∞–≤–∏–º –≤—Å–µ index.html –≤ –ø–æ–¥–ø–∞–ø–∫–∞—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä /category/index.html => /category/)
            for root, dirs, files in os.walk('.'):
                dirs[:] = [d for d in dirs if not d.startswith('.')]
                if 'index.html' in files and root != '.':
                    rel_path = os.path.relpath(root, '.')
                    url_path = '/' + rel_path.replace('\\', '/') + '/'
                    pages.append({
                        'url': url_path,
                        'priority': '0.9',
                        'changefreq': 'weekly'
                    })

            # –î–æ–±–∞–≤–∏–º –≤—Å–µ –æ—Ç–¥–µ–ª—å–Ω—ã–µ .html —Ñ–∞–π–ª—ã (—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–ª–∏ —Å—Ç–∞—Ç—å–∏)
            for root, dirs, files in os.walk('.'):
                dirs[:] = [d for d in dirs if not d.startswith('.')]
                for file in files:
                    if file.endswith('.html') and file not in ['index.html', '404.html']:
                        rel_path = os.path.relpath(os.path.join(root, file), '.')
                        url_path = '/' + rel_path.replace('\\', '/')
                        priority = '0.7' if any(kw in file for kw in [
                            'affiliate-disclosure', 'contact-us', 'disclaimer',
                            'editorial-policy', 'privacy-policy', 'terms-of-use',
                            'sitemap'
                        ]) else '0.8'
                        pages.append({
                            'url': url_path,
                            'priority': priority,
                            'changefreq': 'monthly'
                        })

            # –£–¥–∞–ª–∏–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
            seen = set()
            unique_pages = []
            for p in pages:
                if p['url'] not in seen:
                    unique_pages.append(p)
                    seen.add(p['url'])
            pages = sorted(unique_pages, key=lambda x: float(x['priority']), reverse=True)

            # –°–±–æ—Ä–∫–∞ XML —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
            xml = '''<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xsi:schemaLocation="http://www.sitemaps.org/schemas/sitemap/0.9
        http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd">
'''
            
            for page in pages:
                xml += f'''  <url>
    <loc>{SITE_URL}{page['url']}</loc>
    <lastmod>{today}</lastmod>
    <changefreq>{page['changefreq']}</changefreq>
    <priority>{page['priority']}</priority>
  </url>
'''
            xml += '</urlset>'

            with open('sitemap.xml', 'w', encoding='utf-8') as f:
                f.write(xml)
            print(f"‚úÖ sitemap.xml —Å–æ–∑–¥–∞–Ω: {len(pages)} —Å—Ç—Ä–∞–Ω–∏—Ü")

            # –°–æ–∑–¥–∞–µ–º robots.txt
            with open('robots.txt', 'w', encoding='utf-8') as f:
                f.write(f"User-agent: *\nAllow: /\n\nSitemap: {SITE_URL}/sitemap.xml")
            print("‚úÖ robots.txt —Å–æ–∑–¥–∞–Ω")

        def main():
            print("üöÄ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è sitemap.xml –∏ robots.txt...")
            generate_sitemap()
            print("‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

        if __name__ == "__main__":
            main()
        EOF

        python update_site.py

    - name: Check for changes
      id: check_changes
      run: |
        if git diff --quiet; then
          echo "changed=false" >> $GITHUB_OUTPUT
          echo "No changes detected"
        else
          echo "changed=true" >> $GITHUB_OUTPUT
          echo "Changes detected"
        fi

    - name: Commit and push changes
      if: steps.check_changes.outputs.changed == 'true'
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "GitHub Actions"
        git add sitemap.xml robots.txt
        git commit -m "üîÑ Auto-regenerated sitemap.xml and robots.txt

        üìä Complete sitemap with all 41 pages
        üîß Fixed XML structure with proper <urlset> tags
        ‚úÖ Technical pages priority 0.7, content pages 0.8
        ü§ñ Updated robots.txt with sitemap reference"
        git push

    - name: No changes
      if: steps.check_changes.outputs.changed == 'false'
      run: echo "üü¢ –í—Å—ë –∞–∫—Ç—É–∞–ª—å–Ω–æ. –û–±–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–µ –Ω—É–∂–Ω—ã."
